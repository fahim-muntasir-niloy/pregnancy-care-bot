{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88TaXhDtk_4P",
    "outputId": "1f1671ef-3413-4e11-c755-da25c11dbbd4"
   },
   "outputs": [],
   "source": [
    "# load credentials\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-xKFB_NPkwTP",
    "outputId": "e53532d0-98cf-46f4-91c5-b30a1026836e"
   },
   "outputs": [],
   "source": [
    "# pip install langchain_community tiktoken langchainhub chromadb langchain langchain-huggingface PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "joApSFjZk0c6",
    "outputId": "0c01ce30-16bf-418b-b18b-29a9adcb48a8"
   },
   "outputs": [],
   "source": [
    "# load modules\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "from langchain_community.vectorstores import Chroma, FAISS, AstraDB\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages loaded: 59\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "folder_path = \"F:\\\\LLM_projects\\\\research_paper_bot\\\\web_scraping\"\n",
    "\n",
    "# Get all .txt files in the folder\n",
    "txt_files = [file for file in os.listdir(folder_path) if file.endswith('.txt')]\n",
    "\n",
    "all_pages = []  # To store pages from all files\n",
    "\n",
    "for txt in txt_files:\n",
    "    # print(f\"Processing {txt} file...\")\n",
    "    loader = TextLoader(file_path=os.path.join(folder_path, txt), encoding=\"utf-8\")\n",
    "    \n",
    "    # Load the documents for this file and append to `all_pages`\n",
    "    for doc in loader.lazy_load():\n",
    "        all_pages.append(doc)\n",
    "\n",
    "# Output all collected pages\n",
    "print(f\"Total pages loaded: {len(all_pages)}\")\n",
    "\n",
    "    # txtfile = loader.load()\n",
    "    # print(txtfile[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "oCfdfvZGpNV2"
   },
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# loader = PyMuPDFLoader(\"F:\\\\LLM_projects\\\\research_paper_bot\\\\demo_3.pdf\")\n",
    "\n",
    "# page = []\n",
    "# for doc in loader.lazy_load():\n",
    "#     page.append(doc)\n",
    "#     # if len(page) >= 10:\n",
    "#     #     page = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "MQGfHfyklJFr"
   },
   "outputs": [],
   "source": [
    "# Splitting document\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 512,\n",
    "    chunk_overlap = 200,\n",
    "        separators=[\n",
    "        \"\\n\"]\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(all_pages)\n",
    "# splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymczpJBBquxs",
    "outputId": "d5234b9a-e13b-4c17-953e-409855c98e43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1174"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgd4Cqq3Da0U"
   },
   "source": [
    "# Embedding model: BAAI/bge-m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TWX3qyX1lQng",
    "outputId": "9e7dbeb9-2d2e-4ab3-93f3-682aff821d9c"
   },
   "outputs": [],
   "source": [
    "# model_name = 'BAAI/bge-m3'\n",
    "\n",
    "# # model_kwargs = {\"device\": \"cpu\"}\n",
    "# encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "\n",
    "# vec_db = Chroma.from_documents(\n",
    "#     documents=splits,\n",
    "#     embedding= HuggingFaceBgeEmbeddings(model_name = model_name,\n",
    "#                                         encode_kwargs = encode_kwargs)\n",
    "#     )\n",
    "\n",
    "model_name = \"G:\\\\office\\\\langflow\\\\models--BAAI--bge-m3\\\\snapshots\\\\babcf60cae0a1f438d7ade582983d4ba462303c2\"\n",
    "# model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "hf_embedding_model = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "\n",
    "ASTRA_DB_APPLICATION_TOKEN = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
    "ASTRA_DB_API_ENDPOINT = os.getenv(\"ASTRA_DB_API_ENDPOINT\")\n",
    "ASTRA_DB_KEYSPACE = os.getenv(\"ASTRA_DB_KEYSPACE\")\n",
    "\n",
    "# vec_db = AstraDB.from_documents(    \n",
    "#     collection_name=\"pregnancy_bot\",\n",
    "#     api_endpoint=ASTRA_DB_API_ENDPOINT,\n",
    "#     token=ASTRA_DB_APPLICATION_TOKEN,\n",
    "#     namespace='default_keyspace',\n",
    "    \n",
    "#     documents=splits,\n",
    "#     embedding= hf_embedding_model\n",
    "#     )\n",
    "\n",
    "\n",
    "# cleanup\n",
    "# vec_db.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "APICommander about to raise from: [{'message': \"Collection already exists: trying to create Collection ('pregnancy_bot') with different settings\", 'errorCode': 'EXISTING_COLLECTION_DIFFERENT_SETTINGS', 'id': '8487252e-5ce9-4176-9699-0635672fec4b', 'title': 'Collection already exists', 'family': 'REQUEST', 'scope': 'EMPTY'}]\n",
      "d:\\torch-venv\\lib\\site-packages\\langchain_astradb\\utils\\astradb.py:387: UserWarning: Astra DB collection 'pregnancy_bot' is detected as having indexing turned on for all fields (either created manually or by older versions of this plugin). This implies stricter limitations on the amount of text each string in a document can store. Consider indexing anew on a fresh collection to be able to store longer texts. See https://github.com/langchain-ai/langchain-datastax/blob/main/libs/astradb/README.md#warnings-about-indexing for more details.\n",
      "  if not self._validate_indexing_policy(\n"
     ]
    }
   ],
   "source": [
    "# as data is already loaded\n",
    "from langchain_astradb import AstraDBVectorStore\n",
    "\n",
    "vec_db = AstraDBVectorStore(\n",
    "    embedding=hf_embedding_model,\n",
    "    collection_name=\"pregnancy_bot\",\n",
    "    api_endpoint=ASTRA_DB_API_ENDPOINT,\n",
    "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
    "    namespace='default_keyspace',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "louhvreXlc_k"
   },
   "outputs": [],
   "source": [
    "vec_retriever = vec_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVI7ruhtliqe"
   },
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Usp0l-HIlkk6",
    "outputId": "b22ab10e-2cee-4fde-fbce-b93c8b928bbc"
   },
   "outputs": [],
   "source": [
    "# os.environ[\"API_TOKEN\"] = hf_token\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    top_k=5,\n",
    "    top_p=0.85,\n",
    "    # typical_p=0.9,\n",
    "    temperature=0.4,\n",
    "    max_new_tokens=2048,\n",
    "    do_sample=False,\n",
    "    # repetition_penalty=1.1,\n",
    "    stop_sequences=[\"\\n\\n\"],\n",
    "    huggingfacehub_api_token=HF_TOKEN\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "rJT_UbdylrU1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\torch-venv\\lib\\site-packages\\langsmith\\client.py:354: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prompt setup\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# format context\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": vec_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are a doctor who is answering pregnancy questions.\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "Always reply in bangla text, do not answer in english. No need to translate in english too.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "End the answer on a single । punctuation.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": vec_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: আমি প্রেগনেন্সী কেয়ার বট। গর্ভাবস্থায় যেকোনো জিজ্ঞাসা আমাকে করতে পারেন। আপনাকে কিভাবে সাহায্য করতে পারি?\n",
      "Human: প্রেগনেন্সী টেস্ট কিভাবে করবো?\n",
      "Bot: প্রেগন্যান্সি টেস্ট করার জন্য আপনার নিকটস্থ ফার্মেসি বা সুপারশপ থেকে প্রেগন্যান্সি টেস্ট কিট/বক্স কিনে নিন। দিনের যেকোনো সময়ের প্রস্রাবের নমুনা নিয়ে প্রেগন্যান্সি টেস্ট করা যায়। প্রেগন্যান্সি টেস্ট কিট/স্টিকের উপর প্রস্রাব করার পর সাধারণত কয়েক মিনিটের মধ্যেই ফলাফল দেখায়। \n"
     ]
    }
   ],
   "source": [
    "print(\"Bot: আমি প্রেগনেন্সী কেয়ার বট। গর্ভাবস্থায় যেকোনো জিজ্ঞাসা আমাকে করতে পারেন। আপনাকে কিভাবে সাহায্য করতে পারি?\")\n",
    "qstn = input()\n",
    "print(f\"Human: {qstn}\")\n",
    "response = rag_chain.invoke(qstn)\n",
    "ans = response.split(\"।।\")[0]\n",
    "print(f\"Bot:{ans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " হরমোন খেতে হয় না, গর্ভবতী হলে শরীরে নিজেই হরমোন তৈরি হয়\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"গর্ভবতী হলে হরমোন খেতে হয়?\")\n",
    "print(response.split(\"।।\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: আমি প্রেগনেন্সী কেয়ার বট। গর্ভাবস্থায় যেকোনো জিজ্ঞাসা আমাকে করতে পারেন। আপনাকে কিভাবে সাহায্য করতে পারি?\n",
      "Human: গর্ভাবস্থায় কি কি হরমোন প্রভাব রাখে?\n",
      "Bot: গর্ভাবস্থায় হিউম্যান কোরিওনিক গোনাডোট্রোপিন হরমোন, এস্ট্রোজেন হরমোন, প্রোজেস্টেরোন হরমোন ইত্যাদি হরমোন প্রভাব রাখে\n"
     ]
    }
   ],
   "source": [
    "print(\"Bot: আমি প্রেগনেন্সী কেয়ার বট। গর্ভাবস্থায় যেকোনো জিজ্ঞাসা আমাকে করতে পারেন। আপনাকে কিভাবে সাহায্য করতে পারি?\")\n",
    "qstn = input()\n",
    "print(f\"Human: {qstn}\")\n",
    "response = rag_chain.invoke(qstn)\n",
    "ans = response.split(\"।।\")[0]\n",
    "print(f\"Bot:{ans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SR-RjbARsT7t"
   },
   "source": [
    "# Q/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " প্রেগন্যান্সি টেস্ট কিট কেনা হয়। আগামী সপ্তাহে নির্ধারিত সময়ে মাসিক না হলে আপনি হয়তো চিন্তা করবেন গর্ভধারণ হলো কি না। তখন পরীক্ষা করার জন্য প্রেগন্যান্সি টেস্ট কিনে রাখতে পারেন। প্রেগন্যান্সি টেস্টের মাধ্যমে আপনি গর্ভধারণ করেছেন কি না সেটি নির্ধারণ করা যাবে। \n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"প্রেগনেন্সী টেস্ট কিভাবে করবো?\")\n",
    "print(response.split(\"।।\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " মাসিক শুরু হওয়ার আগেই আবারও গর্ভধারণ সম্ভব। তাই পিরিয়ড হয়নি বলে এখন গর্ভধারণ করার সম্ভাবনা নেই—এটা ভেবে ভুল করবেন না\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"মাসিকের সাথে গর্ভবতী হওয়ার সম্পর্ক কি?\")\n",
    "print(response.split(\"।।\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jb4SUGAQsP-U",
    "outputId": "7316972f-2c63-44af-957c-9b1c7ae2a9a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " প্রেগনেন্ট হলে আপনাকে প্রতিদিন ছয় ধরনের খাবার অবশ্যই খেতে হবে। এই ছয় ধরনের খাবার হলো—আমলকি, আমড়া, জাম, জলপাই, লেবু, জাম্বুরা, কমলা ও মাল্টা। এগুলোতে ভিটামিন সি থাকে। ভিটামিন সি আপনার ও গর্ভের শিশুর ত্বক, রক্তনালী ও হাড় সুস্থ রাখতে সাহায্য করবে।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m rag_chain\u001b[38;5;241m.\u001b[39mstream(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mপ্রেগনেন্ট হলে কী খাওয়া দরকার?\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chunk, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3407\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstream\u001b[39m(\n\u001b[0;32m   3402\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3403\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   3404\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3405\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3406\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3407\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28miter\u001b[39m([\u001b[38;5;28minput\u001b[39m]), config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3394\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m   3389\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   3390\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Input],\n\u001b[0;32m   3391\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   3392\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   3393\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Output]:\n\u001b[1;32m-> 3394\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m   3395\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3396\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform,\n\u001b[0;32m   3397\u001b[0m         patch_config(config, run_name\u001b[38;5;241m=\u001b[39m(config \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m   3398\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3399\u001b[0m     )\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2197\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   2195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2196\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 2197\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   2199\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3357\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[1;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   3354\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3355\u001b[0m         final_pipeline \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mtransform(final_pipeline, config)\n\u001b[1;32m-> 3357\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m final_pipeline\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\langchain_core\\output_parsers\\transform.py:64\u001b[0m, in \u001b[0;36mBaseTransformOutputParser.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28minput\u001b[39m: Iterator[Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage]],\n\u001b[0;32m     51\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m     53\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[T]:\n\u001b[0;32m     54\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transform the input into the output format.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m        The transformed output.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_stream_with_config(\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform, config, run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m     )\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2197\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[1;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   2195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2196\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 2197\u001b[0m         chunk: Output \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   2198\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m   2199\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\langchain_core\\output_parsers\\transform.py:29\u001b[0m, in \u001b[0;36mBaseTransformOutputParser._transform\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Iterator[Union[\u001b[38;5;28mstr\u001b[39m, BaseMessage]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[T]:\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28minput\u001b[39m:\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunk, BaseMessage):\n\u001b[0;32m     31\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result([ChatGeneration(message\u001b[38;5;241m=\u001b[39mchunk)])\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:1431\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m             final \u001b[38;5;241m=\u001b[39m ichunk\n\u001b[0;32m   1430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m got_first_val:\n\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(final, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:576\u001b[0m, in \u001b[0;36mBaseLLM.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    570\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(\n\u001b[0;32m    571\u001b[0m         e,\n\u001b[0;32m    572\u001b[0m         response\u001b[38;5;241m=\u001b[39mLLMResult(\n\u001b[0;32m    573\u001b[0m             generations\u001b[38;5;241m=\u001b[39m[[generation]] \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    574\u001b[0m         ),\n\u001b[0;32m    575\u001b[0m     )\n\u001b[1;32m--> 576\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    578\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]]))\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:560\u001b[0m, in \u001b[0;36mBaseLLM.stream\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    558\u001b[0m generation: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 560\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[0;32m    561\u001b[0m         prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    562\u001b[0m     ):\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:342\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._stream\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream\u001b[39m(\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    335\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    339\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[GenerationChunk]:\n\u001b[0;32m    340\u001b[0m     invocation_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invocation_params(stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 342\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mtext_generation(\n\u001b[0;32m    343\u001b[0m         prompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minvocation_params, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    344\u001b[0m     ):\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;66;03m# identify stop sequence in generated text, if any\u001b[39;00m\n\u001b[0;32m    346\u001b[0m         stop_seq_found: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    347\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m stop_seq \u001b[38;5;129;01min\u001b[39;00m invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\huggingface_hub\\inference\\_common.py:270\u001b[0m, in \u001b[0;36m_stream_text_generation_response\u001b[1;34m(bytes_output_as_lines, details)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Used in `InferenceClient.text_generation`.\"\"\"\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# Parse ServerSentEvents\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m byte_payload \u001b[38;5;129;01min\u001b[39;00m bytes_output_as_lines:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    272\u001b[0m         output \u001b[38;5;241m=\u001b[39m _format_text_generation_stream_output(byte_payload, details)\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\requests\\models.py:869\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[1;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \n\u001b[0;32m    864\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    867\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[0;32m    870\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode\n\u001b[0;32m    871\u001b[0m ):\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\urllib3\\response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\urllib3\\response.py:1206\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1203\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1208\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\torch-venv\\lib\\site-packages\\urllib3\\response.py:1125\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1125\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1126\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1273\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1270\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1271\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ssl.py:1129\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"প্রেগনেন্ট হলে কী খাওয়া দরকার?\"):\n",
    "    print(chunk, end=\"\", flush=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " প্রেগনেন্ট হলে আপনাকে প্রতিদিন ছয় ধরনের খাবার অবশ্যই খেতে হবে। এই ছয় ধরনের খাবার হলো— ফাইবার সমৃদ্ধ শর্করা, স্বাস্থ্যকর তেলযুক্ত খাবার, প্রোটিন সমৃদ্ধ খাবার, ক্যালসিয়াম সমৃদ্ধ খাবার, ভিটামিন সি সমৃদ্ধ ফল, ও স্বাস্থ্যকর পানীয়।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।।\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"প্রেগনেন্ট হলে কি কি খাওয়া যায়?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/tutorials/rag/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
